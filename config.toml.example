# Open FinOps Stack Configuration
# Copy this file to config.toml and update with your values

[project]
# Project-level defaults
name = "open-finops-stack"
# Default data directory for DuckDB and other local storage
data_dir = "./data"

[database]
# Database backend configuration
# Options: "duckdb", "ducklake", "snowflake", "bigquery", "postgresql"
backend = "duckdb"

# DuckDB-specific configuration (default)
[database.duckdb]
database_path = "./data/finops.duckdb"

# DuckLake-specific configuration (uncomment to use)
# [database.ducklake]
# database_path = "./data/finops.ducklake"
# duckdb_path = "./data/finops-ducklake.duckdb"
# compression = "zstd"
# enable_encryption = false
# partition_strategy = "monthly"


# Snowflake-specific configuration (uncomment to use)
# [database.snowflake]
# account = "your-account.snowflakecomputing.com"
# warehouse = "FINOPS_WH"
# database = "FINOPS_DB"
# schema = "AWS_BILLING"
# user = "finops_user"
# role = "FINOPS_ROLE"
# # password via SNOWFLAKE_PASSWORD environment variable

# BigQuery-specific configuration (uncomment to use)
# [database.bigquery]
# project_id = "your-gcp-project"
# dataset = "finops_data"
# location = "US"
# # credentials via GOOGLE_APPLICATION_CREDENTIALS environment variable

# PostgreSQL-specific configuration (uncomment to use)
# [database.postgresql]
# host = "localhost"
# port = 5432
# database = "finops"
# schema = "aws_billing"
# user = "finops_user"
# # password via POSTGRESQL_PASSWORD environment variable

[aws]
# AWS CUR pipeline configuration
# All these values can be overridden via command-line flags

# Dataset/schema name for AWS billing data
dataset_name = "aws_billing"

# Required settings (no defaults)
# bucket = "your-cur-bucket-name"
# prefix = "your-cur-prefix"  
# export_name = "your-cur-export-name"

# Optional settings with defaults
cur_version = "v1"  # Options: "v1" or "v2"
export_format = "csv"  # Options: "csv" or "parquet" (auto-detected if not specified)

# Date range filters (optional - imports all data if not specified)
# start_date = "2024-01"  # Format: YYYY-MM
# end_date = "2024-12"    # Format: YYYY-MM

# Pipeline behavior
reset = false  # Drop existing tables before import

# AWS credentials (optional - uses AWS SDK credential chain if not specified)
# access_key_id = ""
# secret_access_key = ""
# region = "us-east-1"

[azure]
# Azure billing pipeline configuration (future)
# storage_account = ""
# container = ""
# export_name = ""

[gcp]
# GCP billing pipeline configuration (future)
# project_id = ""
# dataset_id = ""
# table_id = ""